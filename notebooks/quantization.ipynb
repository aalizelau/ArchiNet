{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e801b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time \n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('your_model.pth',weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e69290",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "qconfig = get_default_qconfig(\"qnnpack\")\n",
    "qconfig_dict = {\"\": qconfig}\n",
    "\n",
    "model.eval()\n",
    "model.cpu()  # must be on CPU for quantization\n",
    "example_inputs = (next(iter(val_loader))[0],) \n",
    "prepared_model = prepare_fx(model, qconfig_dict,example_inputs)\n",
    "\n",
    "#calibration\n",
    "with torch.inference_mode():\n",
    "    for imgs, _ in val_loader:\n",
    "        imgs = imgs.to('cpu')\n",
    "        prepared_model(imgs) \n",
    "\n",
    "quantized_model = convert_fx(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ece001",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in quantized_model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d098e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"\n",
    "    A simple timer utility for measuring elapsed time in milliseconds.\n",
    "\n",
    "    Supports both GPU and CPU timing:\n",
    "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
    "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
    "\n",
    "    Methods:\n",
    "        start(): Start the timer.\n",
    "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.starter = torch.cuda.Event(enable_timing=True)\n",
    "            self.ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def start(self):\n",
    "        if self.use_cuda:\n",
    "            self.starter.record()\n",
    "        else:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.use_cuda:\n",
    "            self.ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            return self.starter.elapsed_time(self.ender)  # ms\n",
    "        else:\n",
    "            return (time.time() - self.start_time) * 1000  # ms\n",
    "\n",
    "def estimate_latency(model, example_inputs, repetitions=10):\n",
    "    \"\"\"\n",
    "    Returns avg and std inference latency (ms) over given runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    timer = Timer()\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rep in tqdm(range(repetitions), desc=\"Measuring latency\"):\n",
    "            timer.start()\n",
    "            _ = model(example_inputs)\n",
    "            elapsed = timer.stop()\n",
    "            timings[rep] = elapsed\n",
    "\n",
    "    return np.mean(timings), np.std(timings)\n",
    "\n",
    "def estimate_latency_full(model, tag, skip_gpu):\n",
    "    \"\"\"\n",
    "    Prints model latency on GPU and (optionally) CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # estimate latency on CPU\n",
    "    example_input = torch.rand(10, 3, 384, 384).cpu()\n",
    "    model.cpu()\n",
    "    latency_mu, latency_std = estimate_latency(model, example_input)\n",
    "    print(f\"Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")\n",
    "\n",
    "    # estimate latency on GPU\n",
    "    if not skip_gpu and torch.cuda.is_available():\n",
    "        example_input = torch.rand(128, 3, 32, 32).cuda()\n",
    "        model.cuda()\n",
    "        latency_mu, latency_std = estimate_latency(model, example_input)\n",
    "        print(f\"Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_latency_full(model, \"NextViT\", skip_gpu=True)\n",
    "estimate_latency_full(quantized_model, \"NextViT (Quantized)\", skip_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 3, 384, 384).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input,\n",
    "    \"quantized_model.onnx\",         \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=16,\n",
    "    verbose=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
